{"cells":[{"cell_type":"markdown","source":["## FUAM Migration Notebook \n","\n","Since the first published version, we got numerous feedbacks from you.\n","\n","We were listening and we find the right, mature structure for files, activity ids and silver tables.\n","\n","We try to avoid in the future such a migration notebooks in the future releases of FUAM."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8118a73-8a9e-4ae5-88c5-b4addd7cdfaf"},{"cell_type":"code","source":["import pyspark.sql.functions as f"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d183c9a8-5edf-41d7-b7a3-ed4764d311d1"},{"cell_type":"markdown","source":["### Change activity ids to upper case\n","In the new version specific IDs in the activity table have been put to Upper Case to be able to join to other tables in Power BI. This script updates these IDs one time"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7342056d-4aa9-4b6f-8576-f31a58e59518"},{"cell_type":"code","source":["df_activities = spark.sql(\"SELECT * FROM FUAM_Lakehouse.activities\")\n","for co in df_activities.columns:\n","    if co in ['ActivityId','ArtifactId','CapacityId','DashboardId','DataflowId','DatasetId','DatasourceId','FolderObjectId','GatewayId','Id','ItemId','ReportId','UserId','WorkspaceId','','','','','','','','','',]:\n","        df_activities = df_activities.withColumn(co, f.upper(df_activities[co]))\n","df_activities.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(\"activities\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a4dfa14c-c77f-42b4-acaa-254ed4b07fcd"},{"cell_type":"markdown","source":["### Delete silver tables\n","Old silver tables will be deleted, since they are now available in a seperate Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc5a281e-629f-4ad8-a4da-5441f4ca4416"},{"cell_type":"code","source":["tables_to_delete = ['capacities_silver','active_items_silver','tenant_settings_silver','delegated_tenant_settings_overrides_silver','workspaces_silver','capacity_metrics_by_item_kind_by_day_silver','capacity_metrics_by_timepoint_silver','capacity_metrics_by_item_by_operation_by_day_silver']\n","\n","for table in spark.catalog.listTables():\n","    table_name = table[0]\n","    if table_name in tables_to_delete:\n","        print(table_name + ' will be deleted')\n","        spark.sql(\"DROP TABLE \"+ table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"2e4a1f16-f223-4893-a557-53bf7f8ec45b"},{"cell_type":"markdown","source":["### Move old files into new file structure\n","Existing folders in the lakehouse will be moved to the history folder to enable better readability. \n","This will just put into an \"old\" folder. After updating FUAM, the new pipeline will create a \"raw\" folder, where the daily new data will be persisted."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b089b6db-f823-40b3-ab51-883710dcb4b2"},{"cell_type":"code","source":["folders_to_move = ['active_items','activities','capacities','delegated_tenant_settings_overrides','inventory','refreshables','tenant_settings','workspaces']\n","\n","for folder in mssparkutils.fs.ls(\"Files\"):\n","    if folder.name in folders_to_move:\n","        print(folder.name + ' will be moved')\n","        src_path = folder.path\n","        dest_path = src_path.replace('Files/' + folder.name , 'Files/history/' + '/old/' +  folder.name)\n","        print(dest_path)\n","        mssparkutils.fs.mv(src_path, dest_path, True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c813b296-0533-41cd-ae91-618df229c646"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}